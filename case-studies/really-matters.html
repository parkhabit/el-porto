<h2 class='title'>Tech is a vehicle for change</h2>
<h5>Let's make sure we're on the right path</h5>
<p class='body-text'>
    <q>By 2040 there will be over 80 billion connected devices in the world</q> - Ruth Sleeter. 
    For people who love tech this number may seem exciting, to me a software designer/developer working in tech this number is anxiety inducing. 
    Technology is a vehicle for change and with it humans have accomplished great things; our global community is more advanced than ever with technological innovations being made everyday. But tech can also be exclusionary, products are made for the typical user and ‘edge cases’ often get pushed aside or ignored. Our understanding of what is typical is informed by our own biases, and when you have a tech industry that isn’t representative of the society they make products for, this is an issue. 
    Our societies are biased, racist, sexist, and that doesn’t go away with the introduction of technology. Technology should be thought of as a vehicle, if we follow the same path we’ve always walked we’re just going to get there faster. Without addressing biases in the data we use, our products become a reflection of the structural inequalities in our societies.
    With the exponential rise of tech, now more than ever, it is vital that we take time to understand the ways in which tech can divide our societies and perpetuate a system of inequality. By creating awareness we can start choosing the direction of the path. 
</p>
<h5>The path we've walked - structural inequalities are amplified by technoloy</h5>
<p class='body-text'>
    We have numerous examples in technology of how it can be exclusionary and why this is dangerous. In creating products for the typical user as a society we’re generally talking about white, cis gendered, able bodied, wealthy, male, English speaking. This is our default user, not because each one of us is actively choosing this, but because our structural ableism, racism, misogyny has brought us to this conclusion. Tech products are expensive to make and it isn’t a one size fits all situation, people generally make products that cater to the largest possible audience. This often means that users who are minorities get left out or aren’t catered for properly. Examples of this can be seen everywhere from car safety, facial recognition software, to basic website readability<a href='#fn-2'>[2]</a>. Crash test dummies have long been built based on the average male, this makes them considerably less safe for the majority of women<a href='#fn-1'>[1]</a>. In 2015 Google Photos classified computer programmer Jacky Alcine and his girlfriend numerous times as gorillas, and it is not the first facial recognition system that has trouble identifying race correctly. Even by heading to a random website and turning on a screen reader chances are that they have some issue with readability. We become exclusionary when we make products for the ‘norm’ user, and invite our unconscious biases to the table.
</p>
<p class='body-text'>
    Not only can tech exclude minorities in subtle ways by not considering them in product development, but a number of products have been shown to actively enforce entrenched structural biases. Take for example the case of AI gone wrong, within the American justice system an algorithm was created to assess the risk of a defendant reoffending. The concept isn’t in and of itself racist, but the data that it was built on was and resulted in a system that was highly biased against people of colour<a href='#fn-3'>[3]</a>. 
</p>
<h5>It's time to choose a different path - a call to arms for those who work in IT</h5>
<p class='body-text'>
    We are codifying our societies’ biases into everything that we create and to simply be unaware of how it will play out is not good enough. When we think about the scale on which tech has the capacity to operate, these ramifications are only going to become more dire. People aren’t necessarily attempting to build racist, sexist, or ableist products, but when we produce work in an echo chamber, we can end up with products that are discriminatory. So how can we reduce bias within our work in tech? Human Centered Design can help us all start moving in the right direction, but I believe representation is the key. 
</p>
<p class='body-text'>
    Human Centered Design aims to place humans at the center of product formation, not typical users, or the norm user, but people. It works to help us ask questions of the people who we are making products for, centering the creation around them. Although this is well intentioned, we still need to ask ourselves which human do we place at the center, without thinking, our bias leads us to put ourselves at the center or people who look like us at the core of what we do, we design from this. Then we ship it and the internet amplifies once again we can create exclusionary systems without intending to. HCD is on the right track by placing humans and their needs at the center of products, and it can definitely help to reduce unconscious bias. However, I believe what we really need to be doing is asking ourselves who are the people missing from this product room, from this design sprint, from the boardroom?
</p>
<p class='body-text'>
    Let's shoot for the stars and ask that the tech industry become representative of who they’re creating products for. By reducing bias in the hiring process as much as possible, and by creating work environments that are supportive, anti-racist, and vocal about diversity we can hopefully not only hire diverse talent but retain people. We need to have diverse groups of people chairing these product meetings, sitting at the head of the table and inviting others to join. It is my belief that we’re always going to have significant issues with tech products if they are not being designed and built by the people who are using them. 
</p>
<h5>Some groups working towards adding more diversity to the tech industry</h5>
<p class='body-text'>
    <ul>
        <li>Founders and coders</li>
        <li>Code your future</li>
    </ul>
</p>


<ul class='footnotes'>
    <li id='fn-1'>1. <a href='https://www.theguardian.com/lifeandstyle/2019/feb/23/truth-world-built-for-men-car-crashes'>https://www.theguardian.com/lifeandstyle/2019/feb/23/truth-world-built-for-men-car-crashes</a></li>
    <li id='fn-2'>2. <a href='https://www.24a11y.com/2018/how-ableism-leads-to-inaccessibility/'>https://www.24a11y.com/2018/how-ableism-leads-to-inaccessibility/</a></li>
    <li id='fn-3'>3. <a href='https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing'>https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing</a></li>
</ul>